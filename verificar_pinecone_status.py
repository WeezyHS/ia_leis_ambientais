"""
Verificador de Status do Pinecone - Analisa se h√° dados duplicados
Verifica o estado atual do √≠ndice e identifica poss√≠veis duplica√ß√µes
"""

import os
import json
from datetime import datetime
from dotenv import load_dotenv
from pinecone import Pinecone
from langchain_openai import OpenAIEmbeddings
from collections import defaultdict

load_dotenv()

class PineconeStatusChecker:
    """Verifica o status e poss√≠veis duplica√ß√µes no Pinecone"""
    
    def __init__(self):
        self.pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        self.index_name = os.getenv("PINECONE_INDEX_NAME", "leis-ambientais")
        self.index = self.pc.Index(self.index_name)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    def get_index_stats(self) -> dict:
        """Obt√©m estat√≠sticas do √≠ndice"""
        try:
            stats = self.index.describe_index_stats()
            return {
                "total_vectors": stats.total_vector_count,
                "namespaces": stats.namespaces,
                "dimension": stats.dimension,
                "index_fullness": stats.index_fullness
            }
        except Exception as e:
            print(f"‚ùå Erro ao obter estat√≠sticas: {e}")
            return {}
    
    def sample_vectors_by_namespace(self, namespace: str = "", limit: int = 100) -> list:
        """Obt√©m uma amostra de vetores de um namespace"""
        try:
            # Faz uma busca com vetor aleat√≥rio para obter amostras
            dummy_vector = [0.1] * 1536  # Dimens√£o do text-embedding-3-small
            
            response = self.index.query(
                vector=dummy_vector,
                top_k=limit,
                namespace=namespace,
                include_metadata=True
            )
            
            return response.matches
        except Exception as e:
            print(f"‚ùå Erro ao obter amostras do namespace '{namespace}': {e}")
            return []
    
    def analyze_duplicates(self, vectors: list) -> dict:
        """Analisa poss√≠veis duplica√ß√µes nos vetores"""
        analysis = {
            "total_vectors": len(vectors),
            "unique_titles": set(),
            "title_counts": defaultdict(int),
            "id_patterns": defaultdict(int),
            "potential_duplicates": [],
            "chunk_analysis": defaultdict(list)
        }
        
        for vector in vectors:
            metadata = vector.metadata
            vector_id = vector.id
            
            # Analisa t√≠tulos
            titulo = metadata.get('titulo', 'Sem t√≠tulo')
            analysis["unique_titles"].add(titulo)
            analysis["title_counts"][titulo] += 1
            
            # Analisa padr√µes de ID
            if 'lei_' in vector_id and '_chunk_' in vector_id:
                lei_part = vector_id.split('_chunk_')[0]
                analysis["id_patterns"][lei_part] += 1
                analysis["chunk_analysis"][lei_part].append({
                    "id": vector_id,
                    "chunk_index": metadata.get('chunk_index', 0),
                    "titulo": titulo
                })
            
            # Identifica poss√≠veis duplicatas por t√≠tulo
            if analysis["title_counts"][titulo] > 10:  # Muitos chunks do mesmo t√≠tulo
                analysis["potential_duplicates"].append({
                    "titulo": titulo,
                    "count": analysis["title_counts"][titulo],
                    "id": vector_id
                })
        
        return analysis
    
    def check_for_duplicates(self) -> dict:
        """Verifica duplica√ß√µes em todos os namespaces"""
        print("üîç VERIFICANDO DUPLICA√á√ïES NO PINECONE...")
        
        stats = self.get_index_stats()
        if not stats:
            return {"erro": "N√£o foi poss√≠vel obter estat√≠sticas"}
        
        print(f"\nüìä ESTAT√çSTICAS GERAIS:")
        print(f"  Total de vetores: {stats.get('total_vectors', 0):,}")
        print(f"  Dimens√£o: {stats.get('dimension', 0)}")
        print(f"  Ocupa√ß√£o do √≠ndice: {stats.get('index_fullness', 0):.2%}")
        
        namespaces = stats.get('namespaces', {})
        print(f"  Namespaces: {len(namespaces)}")
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "stats": stats,
            "namespace_analysis": {}
        }
        
        # Analisa cada namespace
        for namespace_name, namespace_stats in namespaces.items():
            namespace_display = namespace_name if namespace_name else "default"
            vector_count = namespace_stats.get('vector_count', 0)
            
            print(f"\nüîç Analisando namespace '{namespace_display}' ({vector_count:,} vetores)...")
            
            # Obt√©m amostra de vetores
            sample_vectors = self.sample_vectors_by_namespace(namespace_name, min(vector_count, 200))
            
            if sample_vectors:
                # Analisa duplica√ß√µes
                analysis = self.analyze_duplicates(sample_vectors)
                
                print(f"  üìÑ Amostra analisada: {analysis['total_vectors']} vetores")
                print(f"  üìö T√≠tulos √∫nicos: {len(analysis['unique_titles'])}")
                print(f"  üî¢ Padr√µes de ID √∫nicos: {len(analysis['id_patterns'])}")
                
                # Verifica se h√° muitas duplica√ß√µes
                high_count_titles = [titulo for titulo, count in analysis['title_counts'].items() if count > 15]
                if high_count_titles:
                    print(f"  ‚ö†Ô∏è T√≠tulos com muitos chunks: {len(high_count_titles)}")
                    for titulo in high_count_titles[:3]:  # Mostra apenas os primeiros 3
                        count = analysis['title_counts'][titulo]
                        print(f"    - {titulo}: {count} chunks")
                else:
                    print(f"  ‚úÖ Nenhuma duplica√ß√£o excessiva detectada")
                
                results["namespace_analysis"][namespace_display] = {
                    "vector_count": vector_count,
                    "sample_size": len(sample_vectors),
                    "unique_titles": len(analysis['unique_titles']),
                    "id_patterns": len(analysis['id_patterns']),
                    "high_count_titles": len(high_count_titles),
                    "title_counts": dict(analysis['title_counts']),
                    "chunk_analysis": dict(analysis['chunk_analysis'])
                }
            else:
                print(f"  ‚ùå N√£o foi poss√≠vel obter amostras")
                results["namespace_analysis"][namespace_display] = {
                    "error": "N√£o foi poss√≠vel obter amostras"
                }
        
        return results
    
    def search_for_specific_content(self, query: str, namespace: str = "") -> list:
        """Busca por conte√∫do espec√≠fico para verificar duplica√ß√µes"""
        try:
            query_embedding = self.embeddings.embed_query(query)
            
            response = self.index.query(
                vector=query_embedding,
                top_k=20,
                namespace=namespace,
                include_metadata=True
            )
            
            return response.matches
        except Exception as e:
            print(f"‚ùå Erro na busca: {e}")
            return []
    
    def test_duplicate_search(self) -> dict:
        """Testa busca por poss√≠veis duplica√ß√µes"""
        print("\nüîç TESTANDO BUSCA POR DUPLICA√á√ïES...")
        
        test_queries = [
            "Lei n¬∫ 1307 de 2002",
            "recursos h√≠dricos",
            "licenciamento ambiental",
            "pol√≠tica estadual"
        ]
        
        results = {}
        
        for query in test_queries:
            print(f"\n  Buscando: '{query}'")
            
            # Busca no namespace padr√£o
            matches = self.search_for_specific_content(query, "")
            
            if matches:
                print(f"    Encontrados: {len(matches)} resultados")
                
                # Analisa se h√° duplica√ß√µes
                titles = [match.metadata.get('titulo', 'Sem t√≠tulo') for match in matches]
                unique_titles = set(titles)
                
                print(f"    T√≠tulos √∫nicos: {len(unique_titles)}")
                
                if len(titles) > len(unique_titles) * 2:  # Muitos resultados para poucos t√≠tulos
                    print(f"    ‚ö†Ô∏è Poss√≠vel duplica√ß√£o detectada")
                else:
                    print(f"    ‚úÖ Distribui√ß√£o normal")
                
                results[query] = {
                    "total_matches": len(matches),
                    "unique_titles": len(unique_titles),
                    "titles": list(unique_titles)[:5]  # Primeiros 5 t√≠tulos
                }
            else:
                print(f"    Nenhum resultado encontrado")
                results[query] = {"total_matches": 0}
        
        return results
    
    def generate_report(self, analysis: dict, search_test: dict) -> str:
        """Gera relat√≥rio sobre duplica√ß√µes"""
        report = f"""
=== RELAT√ìRIO DE VERIFICA√á√ÉO DO PINECONE ===

üìä ESTAT√çSTICAS GERAIS:
  ‚Ä¢ Total de vetores: {analysis['stats'].get('total_vectors', 0):,}
  ‚Ä¢ Namespaces: {len(analysis['stats'].get('namespaces', {})):,}
  ‚Ä¢ Ocupa√ß√£o do √≠ndice: {analysis['stats'].get('index_fullness', 0):.2%}

üîç AN√ÅLISE POR NAMESPACE:
"""
        
        for namespace, data in analysis.get('namespace_analysis', {}).items():
            if 'error' in data:
                report += f"  ‚Ä¢ {namespace}: Erro na an√°lise\n"
                continue
            
            vector_count = data.get('vector_count', 0)
            unique_titles = data.get('unique_titles', 0)
            high_count = data.get('high_count_titles', 0)
            
            report += f"  ‚Ä¢ {namespace}: {vector_count:,} vetores, {unique_titles} t√≠tulos √∫nicos\n"
            
            if high_count > 0:
                report += f"    ‚ö†Ô∏è {high_count} t√≠tulos com muitos chunks (poss√≠vel duplica√ß√£o)\n"
            else:
                report += f"    ‚úÖ Distribui√ß√£o normal de chunks\n"
        
        report += f"\nüîç TESTE DE BUSCA POR DUPLICA√á√ïES:\n"
        
        for query, result in search_test.items():
            total = result.get('total_matches', 0)
            unique = result.get('unique_titles', 0)
            
            if total > 0:
                ratio = total / max(unique, 1)
                status = "‚ö†Ô∏è Poss√≠vel duplica√ß√£o" if ratio > 3 else "‚úÖ Normal"
                report += f"  ‚Ä¢ '{query}': {total} resultados, {unique} t√≠tulos √∫nicos - {status}\n"
            else:
                report += f"  ‚Ä¢ '{query}': Nenhum resultado\n"
        
        # Conclus√£o
        total_vectors = analysis['stats'].get('total_vectors', 0)
        if total_vectors > 200:
            report += f"\n‚ö†Ô∏è ATEN√á√ÉO: √çndice com {total_vectors:,} vetores pode indicar duplica√ß√£o.\n"
            report += f"Para 21 leis expandidas, esperamos ~100-150 vetores.\n"
        else:
            report += f"\n‚úÖ CONCLUS√ÉO: Quantidade de vetores parece normal para o conte√∫do.\n"
        
        return report

def main():
    """Fun√ß√£o principal"""
    print("=== VERIFICA√á√ÉO DE STATUS DO PINECONE ===")
    print("Este script ir√°:")
    print("1. Verificar estat√≠sticas do √≠ndice")
    print("2. Analisar poss√≠veis duplica√ß√µes")
    print("3. Testar busca por conte√∫do duplicado")
    print("4. Gerar relat√≥rio de status")
    
    try:
        checker = PineconeStatusChecker()
        
        # Verifica duplica√ß√µes
        analysis = checker.check_for_duplicates()
        
        # Testa busca por duplica√ß√µes
        search_test = checker.test_duplicate_search()
        
        # Gera relat√≥rio
        report = checker.generate_report(analysis, search_test)
        print(report)
        
        # Salva resultados
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f"tests/pinecone_status_{timestamp}.json"
        
        full_results = {
            "analysis": analysis,
            "search_test": search_test,
            "report": report
        }
        
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(full_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nüìÅ Resultados salvos em: {results_file}")
        
    except Exception as e:
        print(f"‚ùå Erro geral: {e}")

if __name__ == "__main__":
    main()